{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install collections\n",
    "#pip install sklearn\n",
    "#pip install requests\n",
    "#pip install fuzzywuzzy\n",
    "#pip install pandas\n",
    "#pip install numpy\n",
    "#pip install nltk\n",
    "#python -m spacy download en_core_web_sm\n",
    "\n",
    "#request.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/antoniodrakes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/antoniodrakes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.cluster import DBSCAN\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#import files\n",
    "#df = pd.read_csv('cleandata_processed.csv', index_col=0)\n",
    "df = pd.read_excel(io ='/Users/antoniodrakes/Desktop/Voxcroft/February.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame head after stemming and lemmatization:\n",
      "                                         Article_URL Article_Date_Published  \\\n",
      "0  https://twitter.com/TimesLIVE/status/176335098...    2024-03-01 01:50:09   \n",
      "1  https://briefly.co.za/business-economy/economy...    2024-03-01 01:49:05   \n",
      "2  https://twitter.com/i_trafficKZN/status/176334...    2024-03-01 01:38:59   \n",
      "3  https://twitter.com/i_trafficKZN/status/176334...    2024-03-01 01:35:45   \n",
      "4  https://briefly.co.za/facts-lifehacks/celebrit...    2024-03-01 01:33:06   \n",
      "\n",
      "                                        Article_Body  \\\n",
      "0  The University of Johannesburg has confirmed t...   \n",
      "1  French economist Gabriel Zucman speaks during ...   \n",
      "2  162725: Stationary Vehicle on N3 Eastbound aft...   \n",
      "3  162726: Stationary Vehicle on N3 Westbound aft...   \n",
      "4  Coco Gauff is an American professional tennis ...   \n",
      "\n",
      "                     Article_Content_People_AI_Model  \\\n",
      "0                                                NaN   \n",
      "1  Gabriel Zucman|Nelson ALMEIDA|Gabriel Zucman|Z...   \n",
      "2                                          Ramp|Ramp   \n",
      "3  Ramp|Cliffdale I/C|Sterkspruit Road|Ramp|Cliff...   \n",
      "4  Coco Gauff|Venus Williams|Coco Gauff's|Coco Ga...   \n",
      "\n",
      "                   Article_Content_Entities_AI_Model Article_Source  \\\n",
      "0  The University of Johannesburg|https://t.co/os...    twitter.com   \n",
      "1  G20|AFP|Group|AFP|the UC Berkeley|Paris School...  briefly.co.za   \n",
      "2                                                NaN    twitter.com   \n",
      "3                                                NaN    twitter.com   \n",
      "4  @cocogauff|Instagram|@cocogauff|Georgia State ...  briefly.co.za   \n",
      "\n",
      "           Voice                            Article_Themes_AI_Model  \\\n",
      "0     Times LIVE  [Primary: Crime, law and justice|51% |Secondar...   \n",
      "1            NaN  [Primary: Economy, business and finance|100% |...   \n",
      "2  i-traffic KZN  [Primary: Disaster, accident and emergency inc...   \n",
      "3  i-traffic KZN  [Primary: Disaster, accident and emergency inc...   \n",
      "4  Bennett Yates  [Primary: Arts, culture, entertainment and med...   \n",
      "\n",
      "  Article_Subject_Keyword_Identified  \\\n",
      "0          Johannesburg|South Africa   \n",
      "1                                NaN   \n",
      "2                                NaN   \n",
      "3                                NaN   \n",
      "4                                NaN   \n",
      "\n",
      "                    Article_Topic_Keyword_Identified  \\\n",
      "0                                  Crime & Terrorism   \n",
      "1         Civil Unrest & Protest|Leadership delivery   \n",
      "2                                  Natural Disasters   \n",
      "3                                  Natural Disasters   \n",
      "4  Civil Unrest & Protest|Human Rights|State Secu...   \n",
      "\n",
      "                                     Processed_Words  \n",
      "0  [univers, johannesburg, confirm, student, inju...  \n",
      "1  [french, economist, gabriel, zucman, speak, pr...  \n",
      "2  [stationari, vehicl, n, eastbound, ramp, ashbu...  \n",
      "3  [stationari, vehicl, n, westbound, ramp, cliff...  \n",
      "4  [coco, gauff, american, profession, tenni, pla...  \n"
     ]
    }
   ],
   "source": [
    "# Initialize NLTK stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Additional custom stopwords\n",
    "additional_stopwords = {\n",
    "    'according', 'actually', 'almost', 'already', 'although', 'always', 'another',\n",
    "    'anything', 'around', 'away', 'believe', 'better', 'business', 'certain',\n",
    "    'comes', 'concerning', 'consider', 'different', 'enough', 'especially',\n",
    "    'everyone', 'everything', 'exactly', 'finally', 'following', 'happens', 'however',\n",
    "    'important', 'includes', 'including', 'information', 'instead', 'involves', \n",
    "    'least', 'maybe', 'might', 'much', 'often', 'once', 'others', 'perhaps', \n",
    "    'possible', 'probably', 'provides', 'rather', 'recent', 'seems', 'several', \n",
    "    'something', 'sometimes', \"https\", \"com\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \n",
    "    \"nine\", \"ten\", \"read\", \"new\", \"old\", \"also\", \"people\", \"person\",\n",
    "    \"comment\", \"first\", \"last\", \"time\", \"said\", \"like\", \"says\", \"could\", \"social\", \"media\",\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \n",
    "    \"september\", \"october\", \"november\", \"december\",\n",
    "    \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\",\n",
    "    \"day\", \"week\", \"month\", \"year\", \"today\", \"tomorrow\", \"yesterday\",\n",
    "    \"get\", \"go\", \"back\", \"make\", \"way\", \"come\", \"keep\", \"take\", \"put\", \n",
    "    \"thing\", \"think\", \"look\", \"see\", \"know\", \"use\", \"want\", \"need\", \n",
    "    \"good\", \"bad\", \"great\", \"best\", \"better\", \"worst\", \"well\", \"much\", \n",
    "    \"little\", \"big\", \"small\", \"large\", \"old\", \"young\", \"experience\"\n",
    "}\n",
    "\n",
    "# Combine NLTK stopwords with additional stopwords\n",
    "all_stopwords = set(stopwords.words('english')).union(additional_stopwords)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in all_stopwords]  # Remove stopwords\n",
    "    return words  # Return list of words instead of joined string\n",
    "\n",
    "\n",
    "# Apply preprocessing to 'Article_Body'\n",
    "df['Processed_Words'] = df['Article_Body'].apply(preprocess_text)\n",
    "\n",
    "# Function to apply stemming to a list of words\n",
    "def stem_words(words):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Function to apply lemmatization to a list of words\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Apply stemming and lemmatization to 'Processed_Words'\n",
    "df['Processed_Words'] = df['Processed_Words'].apply(stem_words)\n",
    "df['Processed_Words'] = df['Processed_Words'].apply(lemmatize_words)\n",
    "\n",
    "# Display the head of the DataFrame to confirm changes\n",
    "print(\"\\nDataFrame head after stemming and lemmatization:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the sum of TF-IDF scores for each word\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "tfidf_scores_df = pd.DataFrame({'Word': feature_names, 'TF-IDF Score': tfidf_scores})\n",
    "\n",
    "# Sort by TF-IDF score\n",
    "tfidf_scores_df = tfidf_scores_df.sort_values(by='TF-IDF Score', ascending=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "#  Plot the top N words by TF-IDF score\n",
    "top_n = 30  # Number of top words to plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(tfidf_scores_df['Word'].head(top_n), tfidf_scores_df['TF-IDF Score'].head(top_n))\n",
    "plt.xlabel('TF-IDF Score')\n",
    "plt.ylabel('Word')\n",
    "plt.title('Top Words by TF-IDF Score')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_URL</th>\n",
       "      <th>Article_Date_Published</th>\n",
       "      <th>Article_Body</th>\n",
       "      <th>Article_Content_People_AI_Model</th>\n",
       "      <th>Article_Content_Entities_AI_Model</th>\n",
       "      <th>Article_Source</th>\n",
       "      <th>Voice</th>\n",
       "      <th>Article_Themes_AI_Model</th>\n",
       "      <th>Article_Subject_Keyword_Identified</th>\n",
       "      <th>Article_Topic_Keyword_Identified</th>\n",
       "      <th>Processed_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/TimesLIVE/status/176335098...</td>\n",
       "      <td>2024-03-01 01:50:09</td>\n",
       "      <td>The University of Johannesburg has confirmed t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The University of Johannesburg|https://t.co/os...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>Times LIVE</td>\n",
       "      <td>[Primary: Crime, law and justice|51% |Secondar...</td>\n",
       "      <td>Johannesburg|South Africa</td>\n",
       "      <td>Crime &amp; Terrorism</td>\n",
       "      <td>[univers, johannesburg, confirm, student, inju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://briefly.co.za/business-economy/economy...</td>\n",
       "      <td>2024-03-01 01:49:05</td>\n",
       "      <td>French economist Gabriel Zucman speaks during ...</td>\n",
       "      <td>Gabriel Zucman|Nelson ALMEIDA|Gabriel Zucman|Z...</td>\n",
       "      <td>G20|AFP|Group|AFP|the UC Berkeley|Paris School...</td>\n",
       "      <td>briefly.co.za</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Primary: Economy, business and finance|100% |...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Civil Unrest &amp; Protest|Leadership delivery</td>\n",
       "      <td>[french, economist, gabriel, zucman, speak, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/i_trafficKZN/status/176334...</td>\n",
       "      <td>2024-03-01 01:38:59</td>\n",
       "      <td>162725: Stationary Vehicle on N3 Eastbound aft...</td>\n",
       "      <td>Ramp|Ramp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>i-traffic KZN</td>\n",
       "      <td>[Primary: Disaster, accident and emergency inc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natural Disasters</td>\n",
       "      <td>[stationari, vehicl, n, eastbound, ramp, ashbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://twitter.com/i_trafficKZN/status/176334...</td>\n",
       "      <td>2024-03-01 01:35:45</td>\n",
       "      <td>162726: Stationary Vehicle on N3 Westbound aft...</td>\n",
       "      <td>Ramp|Cliffdale I/C|Sterkspruit Road|Ramp|Cliff...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>i-traffic KZN</td>\n",
       "      <td>[Primary: Disaster, accident and emergency inc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natural Disasters</td>\n",
       "      <td>[stationari, vehicl, n, westbound, ramp, cliff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://briefly.co.za/facts-lifehacks/celebrit...</td>\n",
       "      <td>2024-03-01 01:33:06</td>\n",
       "      <td>Coco Gauff is an American professional tennis ...</td>\n",
       "      <td>Coco Gauff|Venus Williams|Coco Gauff's|Coco Ga...</td>\n",
       "      <td>@cocogauff|Instagram|@cocogauff|Georgia State ...</td>\n",
       "      <td>briefly.co.za</td>\n",
       "      <td>Bennett Yates</td>\n",
       "      <td>[Primary: Arts, culture, entertainment and med...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Civil Unrest &amp; Protest|Human Rights|State Secu...</td>\n",
       "      <td>[coco, gauff, american, profession, tenni, pla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Article_URL Article_Date_Published  \\\n",
       "0  https://twitter.com/TimesLIVE/status/176335098...    2024-03-01 01:50:09   \n",
       "1  https://briefly.co.za/business-economy/economy...    2024-03-01 01:49:05   \n",
       "2  https://twitter.com/i_trafficKZN/status/176334...    2024-03-01 01:38:59   \n",
       "3  https://twitter.com/i_trafficKZN/status/176334...    2024-03-01 01:35:45   \n",
       "4  https://briefly.co.za/facts-lifehacks/celebrit...    2024-03-01 01:33:06   \n",
       "\n",
       "                                        Article_Body  \\\n",
       "0  The University of Johannesburg has confirmed t...   \n",
       "1  French economist Gabriel Zucman speaks during ...   \n",
       "2  162725: Stationary Vehicle on N3 Eastbound aft...   \n",
       "3  162726: Stationary Vehicle on N3 Westbound aft...   \n",
       "4  Coco Gauff is an American professional tennis ...   \n",
       "\n",
       "                     Article_Content_People_AI_Model  \\\n",
       "0                                                NaN   \n",
       "1  Gabriel Zucman|Nelson ALMEIDA|Gabriel Zucman|Z...   \n",
       "2                                          Ramp|Ramp   \n",
       "3  Ramp|Cliffdale I/C|Sterkspruit Road|Ramp|Cliff...   \n",
       "4  Coco Gauff|Venus Williams|Coco Gauff's|Coco Ga...   \n",
       "\n",
       "                   Article_Content_Entities_AI_Model Article_Source  \\\n",
       "0  The University of Johannesburg|https://t.co/os...    twitter.com   \n",
       "1  G20|AFP|Group|AFP|the UC Berkeley|Paris School...  briefly.co.za   \n",
       "2                                                NaN    twitter.com   \n",
       "3                                                NaN    twitter.com   \n",
       "4  @cocogauff|Instagram|@cocogauff|Georgia State ...  briefly.co.za   \n",
       "\n",
       "           Voice                            Article_Themes_AI_Model  \\\n",
       "0     Times LIVE  [Primary: Crime, law and justice|51% |Secondar...   \n",
       "1            NaN  [Primary: Economy, business and finance|100% |...   \n",
       "2  i-traffic KZN  [Primary: Disaster, accident and emergency inc...   \n",
       "3  i-traffic KZN  [Primary: Disaster, accident and emergency inc...   \n",
       "4  Bennett Yates  [Primary: Arts, culture, entertainment and med...   \n",
       "\n",
       "  Article_Subject_Keyword_Identified  \\\n",
       "0          Johannesburg|South Africa   \n",
       "1                                NaN   \n",
       "2                                NaN   \n",
       "3                                NaN   \n",
       "4                                NaN   \n",
       "\n",
       "                    Article_Topic_Keyword_Identified  \\\n",
       "0                                  Crime & Terrorism   \n",
       "1         Civil Unrest & Protest|Leadership delivery   \n",
       "2                                  Natural Disasters   \n",
       "3                                  Natural Disasters   \n",
       "4  Civil Unrest & Protest|Human Rights|State Secu...   \n",
       "\n",
       "                                     Processed_Words  \n",
       "0  [univers, johannesburg, confirm, student, inju...  \n",
       "1  [french, economist, gabriel, zucman, speak, pr...  \n",
       "2  [stationari, vehicl, n, eastbound, ramp, ashbu...  \n",
       "3  [stationari, vehicl, n, westbound, ramp, cliff...  \n",
       "4  [coco, gauff, american, profession, tenni, pla...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning [Lydia & Molly]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Position and Frequency [Lydia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/9qwt5c010gn_vf99w5rlrp_40000gn/T/ipykernel_17167/1864580914.py:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_new.loc[newdf_counter, 'Position'] = str(position_counter)\n",
      "/var/folders/qj/9qwt5c010gn_vf99w5rlrp_40000gn/T/ipykernel_17167/1864580914.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_new['Name'] = df['Name'].str.upper()\n"
     ]
    }
   ],
   "source": [
    "# to run for whole `dataset use subset = df`\n",
    "#subset = df\n",
    "subset = df[:1000]\n",
    "\n",
    "#create a new empty dataset to add\n",
    "df_new = pd.DataFrame(columns=df.columns)\n",
    "#the column with the name or entity name\n",
    "df_new.rename(columns={'Article_Content_People_AI_Model': 'Name'}, inplace=True)\n",
    "#the type, either people or entity\n",
    "df_new.rename(columns={'Article_Content_Entities_AI_Model': 'Identity_Type'}, inplace=True)\n",
    "#new column with the position of the name relative to the others in the article\n",
    "df_new['Position'] = np.nan\n",
    "#new column with the article associated\n",
    "df_new['Article'] = np.nan\n",
    "#new column with the frequency of a name in an article\n",
    "df_new['Frequency'] = np.nan\n",
    "#new column with Name Cluster\n",
    "df_new['Cluster'] = np.nan\n",
    "#new column with Best Name \n",
    "df_new['Best Name'] = np.nan\n",
    "#new column with Probability Score \n",
    "df_new['Probability Score'] = np.nan\n",
    "\n",
    "#for the regex below\n",
    "import re\n",
    "\n",
    "#separate the strings into arrays\n",
    "for num in range(0, len(subset)):\n",
    "    subset.at[num, 'Article_Content_People_AI_Model'] = str(subset['Article_Content_People_AI_Model'][num]).split(\"|\")\n",
    "    subset.at[num, 'Article_Content_Entities_AI_Model'] = str(subset['Article_Content_Entities_AI_Model'][num]).split(\"|\")\n",
    "    subset.at[num, 'Article_Subject_Keyword_Identified'] = str(subset['Article_Subject_Keyword_Identified'][num]).split(\"|\")\n",
    "    subset.at[num, 'Article_Topic_Keyword_Identified'] = str(subset['Article_Topic_Keyword_Identified'][num]).split(\"|\")\n",
    "    subset.at[num, 'Article_Themes_AI_Model'] = str(subset['Article_Themes_AI_Model'][num]).strip(\"[\")\n",
    "    subset.at[num, 'Article_Themes_AI_Model'] = str(subset['Article_Themes_AI_Model'][num]).strip(\" ]\")\n",
    "    subset.at[num, 'Article_Themes_AI_Model'] = str(subset['Article_Themes_AI_Model'][num]).replace(\"Primary: \", \"\")\n",
    "    subset.at[num, 'Article_Themes_AI_Model'] = str(subset['Article_Themes_AI_Model'][num]).replace(\"Secondary: \", \"\")\n",
    "    subset.at[num, 'Article_Themes_AI_Model'] = re.sub(r'\\|\\d{1,}%', \"\", str(subset['Article_Themes_AI_Model'][num]))\n",
    "    subset.at[num, 'Article_Themes_AI_Model'] = str(subset['Article_Themes_AI_Model'][num]).replace(\"Uncategorized |\", \"\")\n",
    "    subset.at[num, 'Article_Themes_AI_Model'] = str(subset['Article_Themes_AI_Model'][num]).replace(\"|Uncategorized\", \"\")\n",
    "    subset.at[num, 'Article_Themes_AI_Model'] = str(subset['Article_Themes_AI_Model'][num]).replace(\"]\", \"\")\n",
    "    subset.at[num, 'Article_Themes_AI_Model'] = str(subset['Article_Themes_AI_Model'][num]).replace(\"[\", \"|\")\n",
    "    subset.at[num, 'Article_Themes_AI_Model'] = str(subset['Article_Themes_AI_Model'][num]).strip()\n",
    "    subset.at[num, 'Article_Themes_AI_Model'] = str(subset['Article_Themes_AI_Model'][num]).split(\" |\")\n",
    "\n",
    "    #add the people names\n",
    "newdf_counter = 0\n",
    "olddf_counter = -1\n",
    "article_counter = 0\n",
    "for entry in subset['Article_Content_People_AI_Model']:\n",
    "    olddf_counter+=1\n",
    "    position_counter = 0\n",
    "    for word in entry:\n",
    "        if entry.index(word) < position_counter:\n",
    "            df_new.loc[newdf_counter - position_counter + entry.index(word), 'Position']+=(\"|\"+str(position_counter))\n",
    "            break\n",
    "        df_new.loc[newdf_counter, 'Name'] = word\n",
    "        df_new.loc[newdf_counter, 'Identity_Type'] = 'People'\n",
    "        df_new.loc[newdf_counter, 'Position'] = str(position_counter)\n",
    "        df_new.loc[newdf_counter, 'Article'] = article_counter\n",
    "        df_new.loc[newdf_counter, 'Article_Date_Published'] = subset.loc[olddf_counter, 'Article_Date_Published']\n",
    "        df_new.loc[newdf_counter, 'Article_Body'] = subset.loc[olddf_counter, 'Article_Body']\n",
    "        df_new.loc[newdf_counter, 'Article_Source'] = subset.loc[olddf_counter, 'Article_Source']\n",
    "        df_new.loc[newdf_counter, 'Voice'] = subset.loc[olddf_counter, 'Voice']\n",
    "        df_new.at[newdf_counter, 'Article_Themes_AI_Model'] = list(set(subset.at[olddf_counter, 'Article_Themes_AI_Model']))\n",
    "        df_new.at[newdf_counter, 'Article_Subject_Keyword_Identified'] = subset.at[olddf_counter, 'Article_Subject_Keyword_Identified']\n",
    "        df_new.at[newdf_counter, 'Article_Topic_Keyword_Identified'] = subset.at[olddf_counter, 'Article_Topic_Keyword_Identified']\n",
    "        df_new.at[newdf_counter, 'Article_URL'] = subset.at[olddf_counter, 'Article_URL']\n",
    "        newdf_counter+=1\n",
    "        position_counter+=1\n",
    "    article_counter+=1\n",
    "\n",
    "    #add the entity names\n",
    "newdf_counter = len(df_new)\n",
    "olddf_counter = -1\n",
    "article_counter = 0\n",
    "for entry in subset['Article_Content_Entities_AI_Model']:\n",
    "    olddf_counter+=1\n",
    "    position_counter = 0\n",
    "    for word in entry:\n",
    "        if entry.index(word) < position_counter:\n",
    "            df_new.loc[newdf_counter - position_counter + entry.index(word), 'Position']+=(\"|\"+str(position_counter))\n",
    "            break\n",
    "        df_new.loc[newdf_counter, 'Name'] = word\n",
    "        df_new.loc[newdf_counter, 'Identity_Type'] = 'Entity'\n",
    "        df_new.loc[newdf_counter, 'Position'] = str(position_counter)\n",
    "        df_new.loc[newdf_counter, 'Article'] = article_counter\n",
    "        df_new.loc[newdf_counter, 'Article_Date_Published'] = subset.loc[olddf_counter, 'Article_Date_Published']\n",
    "        df_new.loc[newdf_counter, 'Article_Body'] = subset.loc[olddf_counter, 'Article_Body']\n",
    "        df_new.loc[newdf_counter, 'Article_Source'] = subset.loc[olddf_counter, 'Article_Source']\n",
    "        df_new.loc[newdf_counter, 'Voice'] = subset.loc[olddf_counter, 'Voice']\n",
    "        df_new.at[newdf_counter, 'Article_Themes_AI_Model'] = list(set(subset.at[olddf_counter, 'Article_Themes_AI_Model']))\n",
    "        df_new.at[newdf_counter, 'Article_Subject_Keyword_Identified'] = subset.at[olddf_counter, 'Article_Subject_Keyword_Identified']\n",
    "        df_new.at[newdf_counter, 'Article_Topic_Keyword_Identified'] = subset.at[olddf_counter, 'Article_Topic_Keyword_Identified']\n",
    "        df_new.at[newdf_counter, 'Article_URL'] = subset.at[olddf_counter, 'Article_URL']\n",
    "        newdf_counter+=1\n",
    "        position_counter+=1\n",
    "    article_counter+=1\n",
    "\n",
    "    distinct_names = df_new['Name'].unique()\n",
    "\n",
    "for num in range(0, len(df_new)):\n",
    "    df_new.at[num, 'Position'] = str(df_new['Position'][num]).split(\"|\")\n",
    "    df_new.at[num, 'Frequency'] = len(df_new['Position'][num])\n",
    "\n",
    "\n",
    "# Initialize 'Inter Frequency' column with zeros\n",
    "df_new['Inter Frequency'] = np.zeros(len(df_new))\n",
    "\n",
    "# Calculate name counts and merge back into df_new\n",
    "name_counts = df_new['Name'].value_counts()\n",
    "df_new = df_new.merge(name_counts, on='Name', how='left')\n",
    "df_new.rename(columns={'count': 'Name_Count'}, inplace=True)\n",
    "df_new.drop(columns=['Name_Count'], inplace=True)\n",
    "\n",
    "# Extract unique subjects, topics, and themes\n",
    "theme_list = []\n",
    "for entry in df_new['Article_Themes_AI_Model']:\n",
    "    for word in entry:\n",
    "        if word != entry:\n",
    "            theme_list.append(word)\n",
    "unique_themes = list(set(theme_list))\n",
    "\n",
    "subject_list = []\n",
    "for entry in df_new['Article_Subject_Keyword_Identified']:\n",
    "    for word in entry:\n",
    "        if word != 'nan':\n",
    "            subject_list.append(word)\n",
    "unique_subjects = list(set(subject_list))\n",
    "\n",
    "topic_list = []\n",
    "for entry in df_new['Article_Topic_Keyword_Identified']:\n",
    "    for word in entry:\n",
    "        if word != 'nan':\n",
    "            topic_list.append(word)\n",
    "unique_topics = list(set(topic_list))\n",
    "\n",
    "# Function to map topics to numeric keys, need to do this better to generlize\n",
    "def map_topics(topic_list):\n",
    "    for topic in topic_list:\n",
    "        if topic == 'nan':\n",
    "            return\n",
    "        else:\n",
    "            return [unique_topics.index(topic) for topic in topic_list]\n",
    "        \n",
    "def map_subjects(subject_list):\n",
    "    for subject in subject_list:\n",
    "        if subject =='nan':\n",
    "            return\n",
    "        else:\n",
    "            return [unique_subjects.index(subject) for subject in subject_list]\n",
    "\n",
    "def map_themes(theme_list):\n",
    "    for theme in theme_list:\n",
    "        if theme == 'nan':\n",
    "            return\n",
    "        else:\n",
    "            return [unique_themes.index(theme) for theme in theme_list]\n",
    "\n",
    "# Apply the mapping function to 'Topic Key' column\n",
    "df_new['Topic Key'] = df_new['Article_Topic_Keyword_Identified'].apply(map_topics)\n",
    "df_new['Subject Key'] = df_new['Article_Subject_Keyword_Identified'].apply(map_subjects)\n",
    "df_new['Theme Key'] = df_new['Article_Themes_AI_Model'].apply(map_themes)\n",
    "\n",
    "\n",
    "df_new = pd.merge(df_new, df[['Article_Body', 'Processed_Words']], on=['Article_Body' ], how='left')\n",
    "#df_new.drop(columns=['Article_Body'], inplace=True)\n",
    "df_new.rename(columns={'Processed_Words_y': 'Processed_Words'}, inplace=True)\n",
    "\n",
    "\n",
    "# Save the processed DataFrame to CSV\n",
    "#df_new.to_csv('data_processed.csv', index=False)\n",
    "\n",
    "df = df_new\n",
    "\n",
    "\n",
    "#remove link names\n",
    "df= df[~df['Name'].str.startswith('HTTPS', na=False)]\n",
    "\n",
    "def clean_name(name):\n",
    "    if 'https' in name or '.com' in name or '.za' in name:\n",
    "        name = 'nan'\n",
    "\n",
    "#remove NaN names\n",
    "df_new['Name'] = df['Name'].apply(clean_name)\n",
    "df_new = df[df['Name']!='nan'] \n",
    "\n",
    "df_new['Name'] = df['Name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_URL</th>\n",
       "      <th>Article_Date_Published</th>\n",
       "      <th>Article_Body</th>\n",
       "      <th>Name</th>\n",
       "      <th>Identity_Type</th>\n",
       "      <th>Article_Source</th>\n",
       "      <th>Voice</th>\n",
       "      <th>Article_Themes_AI_Model</th>\n",
       "      <th>Article_Subject_Keyword_Identified</th>\n",
       "      <th>Article_Topic_Keyword_Identified</th>\n",
       "      <th>...</th>\n",
       "      <th>Article</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Best Name</th>\n",
       "      <th>Probability Score</th>\n",
       "      <th>Inter Frequency</th>\n",
       "      <th>Topic Key</th>\n",
       "      <th>Subject Key</th>\n",
       "      <th>Theme Key</th>\n",
       "      <th>Processed_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>HULLE</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>MELVIN MAXIM</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>LELITHA JANTJIES</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>GEORGE</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>MELVIN</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>LELITHA</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>SLEGS JOU</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>WENKE OOR MODE</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>KRY</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://twitter.com/DecideDied/status/17598058...</td>\n",
       "      <td>2024-02-20 07:02:58</td>\n",
       "      <td>Cyril Ramaphosa President of South Africa and ...</td>\n",
       "      <td>CYRIL RAMAPHOSA</td>\n",
       "      <td>People</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>DecideDied</td>\n",
       "      <td>[Politics , Law enforcement, Social problem, S...</td>\n",
       "      <td>[ANC, Cyril Ramaphosa, South Africa]</td>\n",
       "      <td>[Crime &amp; Terrorism]</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[0, 10, 6]</td>\n",
       "      <td>[67, 91, 123, 39, 137]</td>\n",
       "      <td>[cyril, ramaphosa, presid, south, africa, lead...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Article_URL Article_Date_Published  \\\n",
       "0  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "1  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "2  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "3  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "4  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "5  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "6  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "7  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "8  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "9  https://twitter.com/DecideDied/status/17598058...    2024-02-20 07:02:58   \n",
       "\n",
       "                                        Article_Body              Name  \\\n",
       "0  01 Mrt.\\n            Jy sou dalk dink hulle he...             HULLE   \n",
       "1  01 Mrt.\\n            Jy sou dalk dink hulle he...      MELVIN MAXIM   \n",
       "2  01 Mrt.\\n            Jy sou dalk dink hulle he...  LELITHA JANTJIES   \n",
       "3  01 Mrt.\\n            Jy sou dalk dink hulle he...            GEORGE   \n",
       "4  01 Mrt.\\n            Jy sou dalk dink hulle he...            MELVIN   \n",
       "5  01 Mrt.\\n            Jy sou dalk dink hulle he...           LELITHA   \n",
       "6  01 Mrt.\\n            Jy sou dalk dink hulle he...         SLEGS JOU   \n",
       "7  01 Mrt.\\n            Jy sou dalk dink hulle he...    WENKE OOR MODE   \n",
       "8  01 Mrt.\\n            Jy sou dalk dink hulle he...               KRY   \n",
       "9  Cyril Ramaphosa President of South Africa and ...   CYRIL RAMAPHOSA   \n",
       "\n",
       "  Identity_Type Article_Source           Voice  \\\n",
       "0        People  netwerk24.com  Pieter Van Zyl   \n",
       "1        People  netwerk24.com  Pieter Van Zyl   \n",
       "2        People  netwerk24.com  Pieter Van Zyl   \n",
       "3        People  netwerk24.com  Pieter Van Zyl   \n",
       "4        People  netwerk24.com  Pieter Van Zyl   \n",
       "5        People  netwerk24.com  Pieter Van Zyl   \n",
       "6        People  netwerk24.com  Pieter Van Zyl   \n",
       "7        People  netwerk24.com  Pieter Van Zyl   \n",
       "8        People  netwerk24.com  Pieter Van Zyl   \n",
       "9        People    twitter.com      DecideDied   \n",
       "\n",
       "                             Article_Themes_AI_Model  \\\n",
       "0                                              [nan]   \n",
       "1                                              [nan]   \n",
       "2                                              [nan]   \n",
       "3                                              [nan]   \n",
       "4                                              [nan]   \n",
       "5                                              [nan]   \n",
       "6                                              [nan]   \n",
       "7                                              [nan]   \n",
       "8                                              [nan]   \n",
       "9  [Politics , Law enforcement, Social problem, S...   \n",
       "\n",
       "     Article_Subject_Keyword_Identified  \\\n",
       "0                                 [nan]   \n",
       "1                                 [nan]   \n",
       "2                                 [nan]   \n",
       "3                                 [nan]   \n",
       "4                                 [nan]   \n",
       "5                                 [nan]   \n",
       "6                                 [nan]   \n",
       "7                                 [nan]   \n",
       "8                                 [nan]   \n",
       "9  [ANC, Cyril Ramaphosa, South Africa]   \n",
       "\n",
       "           Article_Topic_Keyword_Identified  ... Article Frequency  Cluster  \\\n",
       "0  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "1  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "2  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "3  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "4  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "5  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "6  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "7  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "8  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "9                       [Crime & Terrorism]  ...     1.0       2.0      NaN   \n",
       "\n",
       "   Best Name  Probability Score  Inter Frequency  Topic Key  Subject Key  \\\n",
       "0        NaN                NaN              0.0     [5, 1]         None   \n",
       "1        NaN                NaN              0.0     [5, 1]         None   \n",
       "2        NaN                NaN              0.0     [5, 1]         None   \n",
       "3        NaN                NaN              0.0     [5, 1]         None   \n",
       "4        NaN                NaN              0.0     [5, 1]         None   \n",
       "5        NaN                NaN              0.0     [5, 1]         None   \n",
       "6        NaN                NaN              0.0     [5, 1]         None   \n",
       "7        NaN                NaN              0.0     [5, 1]         None   \n",
       "8        NaN                NaN              0.0     [5, 1]         None   \n",
       "9        NaN                NaN              0.0        [5]   [0, 10, 6]   \n",
       "\n",
       "                Theme Key                                    Processed_Words  \n",
       "0                    None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "1                    None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "2                    None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "3                    None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "4                    None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "5                    None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "6                    None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "7                    None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "8                    None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "9  [67, 91, 123, 39, 137]  [cyril, ramaphosa, presid, south, africa, lead...  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head(10)\n",
    "#df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Clusters [molly]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def similarity_matrix(names):\n",
    "    size = len(names)\n",
    "    matrix = np.zeros((size, size))\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if i != j:\n",
    "                matrix[i][j] = fuzz.token_set_ratio(names[i], names[j])\n",
    "    return matrix\n",
    "\n",
    "names = df['Name'].tolist()\n",
    "sim_matrix = similarity_matrix(names)\n",
    "\n",
    "# Clustering using DBSCAN\n",
    "dbscan = DBSCAN(eps=15, min_samples=1, metric='precomputed')\n",
    "clusters = dbscan.fit_predict(100 - sim_matrix)\n",
    "\n",
    "# Ensure the length of clusters matches the length of the dataframe\n",
    "if len(clusters) != len(df):\n",
    "    raise ValueError(f\"Length of clusters ({len(clusters)}) does not match length of dataframe ({len(df)})\")\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "df['Cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within Article Lableing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contains [Madison] -  Dataset_Contains_Part_of_Score, Article_Contains_Part_of_Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to calculate the number of containments within the same article, negatively scored\n",
    "def contains_part_of_within_article(name, names_list):\n",
    "    score = 0\n",
    "    for other_name in names_list:\n",
    "        if pd.isna(name) or pd.isna(other_name) or name == other_name:\n",
    "            continue\n",
    "        if name in other_name:\n",
    "            score -= 1  # Negative scoring for each containment\n",
    "    return score\n",
    "\n",
    "# Define a function to calculate the number of containments across the entire dataset, negatively scored\n",
    "def contains_part_of_across_dataset(name, names_list):\n",
    "    score = 0\n",
    "    for other_name in names_list:\n",
    "        if pd.isna(name) or pd.isna(other_name) or name == other_name:\n",
    "            continue\n",
    "        if name in other_name:\n",
    "            score -= 1  # Negative scoring for each containment\n",
    "    return score\n",
    "\n",
    "# Applying the scoring function within each article\n",
    "df['Article_Contains_Part_of_Score'] = df.groupby('Article')['Name'].transform(lambda x: x.apply(lambda name: contains_part_of_within_article(name, x)))\n",
    "\n",
    "# Applying the scoring function across the dataset\n",
    "all_names = df['Name'].tolist()\n",
    "df['Dataset_Contains_Part_of_Score'] = df['Name'].apply(lambda name: contains_part_of_across_dataset(name, all_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Name [Madison] temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Name [Madison] temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#ai = pd.read_excel(io ='/Users/antoniodrakes/Desktop/Voxcroft/new_data.xlsx')\n",
    "ai = pd.read_csv('ai_best_name_confidence.csv')\n",
    "\n",
    "# Reset index for both dataframes\n",
    "ai.reset_index(inplace=True)\n",
    "\n",
    "# Ensure the names are in all caps\n",
    "df['Name'] = df['Name'].str.upper()\n",
    "ai['Best Name'] = ai['Best Name'].str.upper()\n",
    "\n",
    "# Merge dataframes based on their indices\n",
    "merged_df = pd.merge(df, ai[['index', 'Best Name', 'Probability Score']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# default value\n",
    "\n",
    "# Save merged dataframe to a new CSV file\n",
    "merged_df.to_csv('merged_data.csv', index=False)\n",
    "\n",
    "merged_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df['Probability Score_y'] = merged_df['Probability Score'].str.rstrip('%').astype('float') / 100.0\n",
    "#merged_df['is_true'] = merged_df['Probability Score_y'].apply(lambda x: 1 if x >= 0.80 else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df.head()\n",
    "\n",
    "#df = merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiData [Wiki Data ID & Alternate Names] [Antonio]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_wikidata_info(names):\n",
    "    search_url = 'https://www.wikidata.org/w/api.php'\n",
    "    batch_size = 10  # Adjust batch size based on performance and API rate limits\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(names), batch_size):\n",
    "        batch_names = names[i:i + batch_size]\n",
    "        batch_results = []\n",
    "\n",
    "        for name in batch_names:\n",
    "            search_params = {\n",
    "                'action': 'wbsearchentities',\n",
    "                'format': 'json',\n",
    "                'search': name,\n",
    "                'language': 'en'\n",
    "            }\n",
    "            try:\n",
    "                search_response = requests.get(search_url, params=search_params)\n",
    "                if search_response.status_code == 200:\n",
    "                    search_data = search_response.json()\n",
    "                else:\n",
    "                    batch_results.append((name, 'N/A', []))\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while searching for {name}: {e}\")\n",
    "                batch_results.append((name, 'N/A', []))\n",
    "                continue\n",
    "\n",
    "            # Check if there are search results\n",
    "            if 'search' not in search_data or not search_data['search']:\n",
    "                batch_results.append((name, 'N/A', []))\n",
    "                continue\n",
    "\n",
    "            # Get the first search result containing the name\n",
    "            matched_result = None\n",
    "            for result in search_data['search']:\n",
    "                label = result.get('label', '').lower()\n",
    "                if all(word in label for word in name.lower().split()):\n",
    "                    matched_result = result\n",
    "                    break\n",
    "\n",
    "            # If no exact match is found, take the first result\n",
    "            if not matched_result:\n",
    "                matched_result = search_data['search'][0]\n",
    "\n",
    "            if not matched_result:\n",
    "                batch_results.append((name, 'N/A', []))\n",
    "                continue\n",
    "\n",
    "            wikidata_id = matched_result.get('id', 'N/A')\n",
    "\n",
    "            # Fetch aliases for the entity\n",
    "            item_params = {\n",
    "                'action': 'wbgetentities',\n",
    "                'format': 'json',\n",
    "                'ids': wikidata_id,\n",
    "                'props': 'aliases',\n",
    "                'languages': 'en'\n",
    "            }\n",
    "            try:\n",
    "                item_response = requests.get(search_url, params=item_params)\n",
    "                if item_response.status_code == 200:\n",
    "                    item_data = item_response.json()\n",
    "                    aliases = item_data.get('entities', {}).get(wikidata_id, {}).get('aliases', {}).get('en', [])\n",
    "                    alias_names = [alias['value'] for alias in aliases]\n",
    "                    batch_results.append((name, wikidata_id, alias_names))\n",
    "                else:\n",
    "                    batch_results.append((name, wikidata_id, []))\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while fetching item for {name}: {e}\")\n",
    "                batch_results.append((name, wikidata_id, []))\n",
    "\n",
    "        results.extend(batch_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Fetch Wikidata information in batches\n",
    "wikidata_info = get_wikidata_info(distinct_names)\n",
    "\n",
    "# Create a DataFrame with the fetched information\n",
    "wikidata_df = pd.DataFrame(wikidata_info, columns=['Name', 'Wikidata_ID', 'Alternate_Names'])\n",
    "\n",
    "# Merge the new DataFrame with the original one based on the 'Name' column\n",
    "df = df.merge(wikidata_df, on='Name', how='left')\n",
    "\n",
    "# Ensure 'Alternate_Names' exists and process it\n",
    "if 'Alternate_Names' in df.columns:\n",
    "    df['Alternate_Names'] = df['Alternate_Names'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "else:\n",
    "    df['Alternate_Names'] = ''\n",
    "\n",
    "# Show the updated DataFrame\n",
    "print(df_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #merged_df.drop(columns=['Wikidata_ID_x', 'Alternate_Names_x','Alternate_Names_y','Wikidata_ID_y','index_y'], inplace=True) \n",
    "#df.drop(columns=['Wikidata_ID', 'Alternate_Names'], inplace=True) \n",
    "#merged_df.drop(columns=[\"index_y\",\"index_x\"], inplace=True) \n",
    "#merged_df.drop(columns=['Probability Score_x', 'Best Name_x'], inplace=True) \n",
    "wiki['Name'] = wiki['Name'].str.upper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#temp\n",
    "wiki = pd.read_csv('wikidata.csv')\n",
    "wiki.reset_index(inplace=True)\n",
    "\n",
    "merged_df = pd.merge(df, wiki[['index', 'Alternate_Names', 'Wikidata_ID']], left_index=True,right_index=True ,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERARTICLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate similarity scores for all pairs\n",
    "pairs = []\n",
    "for i in range(len(df)):\n",
    "    for j in range(i + 1, len(df)):\n",
    "        name1 = df.iloc[i]['Name']\n",
    "        name2 = df.iloc[j]['Name']\n",
    "        score = fuzz.ratio(name1, name2)\n",
    "        pairs.append((name1, name2, score))\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=['Name1', 'Name2', 'Score'])\n",
    "\n",
    "# Determine a threshold for matches (e.g., score >= 85)\n",
    "threshold = 75\n",
    "pairs_df['Match'] = pairs_df['Score'] >= threshold\n",
    "\n",
    "print(pairs_df)\n",
    "\n",
    "def print_matches(pairs_df):\n",
    "    matches = pairs_df[pairs_df['Match']]\n",
    "    print(matches)\n",
    "\n",
    "# Print only the pairs that are matches\n",
    "true_matches = print_matches(pairs_df)\n",
    "\n",
    "matches = pairs_df[pairs_df['Match']]\n",
    "\n",
    "matches.head(50)\n",
    "# Print only the pairs that are matches\n",
    "\n",
    "names = df['Name'].unique()\n",
    "similarity_matrix = np.zeros((len(names), len(names)))\n",
    "\n",
    "for i in range(len(names)):\n",
    "    for j in range(len(names)):\n",
    "        if i != j:\n",
    "            similarity_matrix[i, j] = fuzz.ratio(names[i], names[j])\n",
    "\n",
    "# Convert similarity matrix to distance matrix (1 - similarity)\n",
    "distance_matrix = 100 - similarity_matrix\n",
    "\n",
    "# Adjusted DBSCAN clustering\n",
    "clustering = DBSCAN(eps=30, min_samples=20, metric='precomputed').fit(distance_matrix)\n",
    "\n",
    "# Check the labels again\n",
    "print(\"Clustering Labels and Names:\")\n",
    "for label, name in zip(clustering.labels_, names):\n",
    "    print(f\"Label: {label}, Name: {name}\")\n",
    "\n",
    "    # Create a dictionary to map clusters to names\n",
    "name_mappings = {}\n",
    "for cluster_label in set(clustering.labels_):\n",
    "    if cluster_label != -1:  # Exclude noise points\n",
    "        cluster_names = names[clustering.labels_ == cluster_label]\n",
    "        if cluster_names.size > 0:\n",
    "            standardized_name = max(cluster_names, key=len)  # Choose the longest name as the standard\n",
    "            for name in cluster_names:\n",
    "                name_mappings[name] = standardized_name\n",
    "\n",
    "# Apply the name mappings to the dataframe\n",
    "df['Mapped_Name'] = df['Name'].apply(lambda x: name_mappings.get(x, x))\n",
    "\n",
    "print(\"Name Mappings:\")\n",
    "for original_name, mapped_name in name_mappings.items():\n",
    "    print(f\"{original_name} -> {mapped_name}\")\n",
    "\n",
    "print(\"\\nDataFrame with Mapped Names:\")\n",
    "print(df[['Name', 'Mapped_Name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df\n",
    "#print(\"Columns in merged_df:\", merged_df.columns.tolist())\n",
    "#merged_df.drop(columns=['Wikidata_ID', 'Alternate_Names'], inplace=True) \n",
    "#merged_df = pd.merge(df, wiki[['Name', 'Alternate_Names', 'Wikidata_ID']], on =\"Name\"  ,how='left')\n",
    "#merged_df = merged_df.drop_duplicates(subset=['Name', 'Article_Body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('february_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_URL</th>\n",
       "      <th>Article_Date_Published</th>\n",
       "      <th>Article_Body</th>\n",
       "      <th>Name</th>\n",
       "      <th>Identity_Type</th>\n",
       "      <th>Article_Source</th>\n",
       "      <th>Voice</th>\n",
       "      <th>Article_Themes_AI_Model</th>\n",
       "      <th>Article_Subject_Keyword_Identified</th>\n",
       "      <th>Article_Topic_Keyword_Identified</th>\n",
       "      <th>...</th>\n",
       "      <th>Article</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Best Name</th>\n",
       "      <th>Probability Score</th>\n",
       "      <th>Inter Frequency</th>\n",
       "      <th>Topic Key</th>\n",
       "      <th>Subject Key</th>\n",
       "      <th>Theme Key</th>\n",
       "      <th>Processed_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>Hulle</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>Melvin Maxim</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>Lelitha Jantjies</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>George</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/nuus/hy-h...</td>\n",
       "      <td>2024-02-21 02:00:00</td>\n",
       "      <td>01 Mrt.\\n            Jy sou dalk dink hulle he...</td>\n",
       "      <td>Melvin</td>\n",
       "      <td>People</td>\n",
       "      <td>netwerk24.com</td>\n",
       "      <td>Pieter Van Zyl</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[Crime &amp; Terrorism, Leadership delivery]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[mrt, jy, sou, dalk, dink, hull, het, nie, vee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Article_URL Article_Date_Published  \\\n",
       "0  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "1  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "2  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "3  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "4  https://www.netwerk24.com/huisgenoot/nuus/hy-h...    2024-02-21 02:00:00   \n",
       "\n",
       "                                        Article_Body              Name  \\\n",
       "0  01 Mrt.\\n            Jy sou dalk dink hulle he...             Hulle   \n",
       "1  01 Mrt.\\n            Jy sou dalk dink hulle he...      Melvin Maxim   \n",
       "2  01 Mrt.\\n            Jy sou dalk dink hulle he...  Lelitha Jantjies   \n",
       "3  01 Mrt.\\n            Jy sou dalk dink hulle he...            George   \n",
       "4  01 Mrt.\\n            Jy sou dalk dink hulle he...            Melvin   \n",
       "\n",
       "  Identity_Type Article_Source           Voice Article_Themes_AI_Model  \\\n",
       "0        People  netwerk24.com  Pieter Van Zyl                   [nan]   \n",
       "1        People  netwerk24.com  Pieter Van Zyl                   [nan]   \n",
       "2        People  netwerk24.com  Pieter Van Zyl                   [nan]   \n",
       "3        People  netwerk24.com  Pieter Van Zyl                   [nan]   \n",
       "4        People  netwerk24.com  Pieter Van Zyl                   [nan]   \n",
       "\n",
       "  Article_Subject_Keyword_Identified  \\\n",
       "0                              [nan]   \n",
       "1                              [nan]   \n",
       "2                              [nan]   \n",
       "3                              [nan]   \n",
       "4                              [nan]   \n",
       "\n",
       "           Article_Topic_Keyword_Identified  ... Article Frequency  Cluster  \\\n",
       "0  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "1  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "2  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "3  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "4  [Crime & Terrorism, Leadership delivery]  ...     0.0       1.0      NaN   \n",
       "\n",
       "   Best Name  Probability Score  Inter Frequency  Topic Key  Subject Key  \\\n",
       "0        NaN                NaN              0.0     [5, 1]         None   \n",
       "1        NaN                NaN              0.0     [5, 1]         None   \n",
       "2        NaN                NaN              0.0     [5, 1]         None   \n",
       "3        NaN                NaN              0.0     [5, 1]         None   \n",
       "4        NaN                NaN              0.0     [5, 1]         None   \n",
       "\n",
       "  Theme Key                                    Processed_Words  \n",
       "0      None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "1      None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "2      None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "3      None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "4      None  [mrt, jy, sou, dalk, dink, hull, het, nie, vee...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7415, 22)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
