{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jueTxA9P6Wfl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('cleandata_processed.csv', index_col=0, nrows=19351)\n",
        "\n",
        "# Ensure NLTK data is downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize NLTK stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Additional custom stopwords\n",
        "additional_stopwords = {\n",
        "    'according', 'actually', 'almost', 'already', 'although', 'always', 'another',\n",
        "    'anything', 'around', 'away', 'believe', 'better', 'business', 'certain',\n",
        "    'comes', 'concerning', 'consider', 'different', 'enough', 'especially',\n",
        "    'everyone', 'everything', 'exactly', 'finally', 'following', 'happens', 'however',\n",
        "    'important', 'includes', 'including', 'information', 'instead', 'involves',\n",
        "    'least', 'maybe', 'might', 'much', 'often', 'once', 'others', 'perhaps',\n",
        "    'possible', 'probably', 'provides', 'rather', 'recent', 'seems', 'several',\n",
        "    'something', 'sometimes', \"https\", \"com\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
        "    \"nine\", \"ten\", \"read\", \"new\", \"old\", \"also\", \"people\", \"person\",\n",
        "    \"comment\", \"first\", \"last\", \"time\", \"said\", \"like\", \"says\", \"could\", \"social\", \"media\",\n",
        "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\",\n",
        "    \"september\", \"october\", \"november\", \"december\",\n",
        "    \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\",\n",
        "    \"day\", \"week\", \"month\", \"year\", \"today\", \"tomorrow\", \"yesterday\",\n",
        "    \"get\", \"go\", \"back\", \"make\", \"way\", \"come\", \"keep\", \"take\", \"put\",\n",
        "    \"thing\", \"think\", \"look\", \"see\", \"know\", \"use\", \"want\", \"need\",\n",
        "    \"good\", \"bad\", \"great\", \"best\", \"better\", \"worst\", \"well\", \"much\",\n",
        "    \"little\", \"big\", \"small\", \"large\", \"old\", \"young\", \"experience\"\n",
        "}\n",
        "\n",
        "# Combine NLTK stopwords with additional stopwords\n",
        "all_stopwords = set(stopwords.words('english')).union(additional_stopwords)\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in all_stopwords]  # Remove stopwords\n",
        "    return words  # Return list of words instead of joined string\n",
        "\n",
        "# Apply preprocessing to 'Article_Body'\n",
        "df['Processed_Words'] = df['Article_Body'].apply(preprocess_text)\n",
        "\n",
        "# Function to apply stemming to a list of words\n",
        "def stem_words(words):\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Function to apply lemmatization to a list of words\n",
        "def lemmatize_words(words):\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Apply stemming and lemmatization to 'Processed_Words'\n",
        "df['Processed_Words'] = df['Processed_Words'].apply(stem_words)\n",
        "df['Processed_Words'] = df['Processed_Words'].apply(lemmatize_words)\n",
        "\n",
        "# Convert 'Processed_Words' back to string format for TF-IDF vectorizer\n",
        "df['Processed_Words_Str'] = df['Processed_Words'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Compute TF-IDF scores\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "tfidf_matrix = vectorizer.fit_transform(df['Processed_Words_Str'])\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Identify the most relevant words for each article\n",
        "df['Top_Words'] = tfidf_df.apply(lambda x: [feature_names[i] for i in x.argsort()[-10:][::-1]], axis=1)\n",
        "\n",
        "# Print the top words for each article\n",
        "print(\"\\nTop words for each article:\")\n",
        "print(df[['Article_Body', 'Top_Words']].head())\n",
        "\n",
        "# Aggregate TF-IDF scores to find the most important words across all articles\n",
        "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
        "tfidf_scores_df = pd.DataFrame({'Word': feature_names, 'TF-IDF Score': tfidf_scores})\n",
        "\n",
        "# Sort by TF-IDF score\n",
        "tfidf_scores_df = tfidf_scores_df.sort_values(by='TF-IDF Score', ascending=False)\n"
      ]
    }
  ]
}